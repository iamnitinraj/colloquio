Ques,Ans
   Tell me about yourself?," My name is Susan Klein and I am a Product Manager at Company XYZ. I have previously worked as an Assistant Product Manager at both Company ABC and Company DEF. I’ve done a Degree in Product Management.Out of my 8-year work experience, my greatest accomplishment has been to design, test and launch products in two foreign markets three months ahead of schedule and 25% under budget.I have made it my personal goal to take time to build solid long lasting business relationships with international suppliers, vendors and partners.I have even taught myself conversational foreign languages to break communication and trust barriers. I take pride in being able to negotiate production and distribution deals that save a lot of costs.I will bring the same expertise to your Company as a Product Director, in your quest to launch new product lines in international markets.  !@#@! I grew up in a small town in upstate New York most people have never heard of, but I know you’ll recognize it because I saw it listed as your hometown on your employee bio! What a small world. I graduated from ABC University one year ago, then moved here to be an executive assistant at XYZ Organization. It fit well with my passion for analytics and conceptualizing creative campaigns. Now, I’m looking for my next professional challenge, and believe I’ve found it at your company. !@#@!  Currently, I serve as the assistant to three of the company’s five executive team members including the CEO. During my time at the organisation, I have been recognised for my time management skills, writing abilities and commitment to excellence. !@#@!  I am a vigilant and proactive Security Officer working to ensure safe, secure and orderly environments. I’m also a lifelong learner always seeking out the latest security equipment and techniques to patrol buildings. Lastly, I am thorough in documenting all incidents and actively making suggestions to management about security improvements and changes.  !@#@!  Good morning afternoon evening  sir/mam. First of all, thank you for giving me this opportunity to introduce myself. My name is Ajeet Kumar. As far as my education qualification is concerned, I have done MBA with finance stream from Srivenkateswara university in Emerald's P. G. College, Tirupathi, in the year of 2014. I had completed B.tech from N.I.T Jaipur in 2012. I had completed my schooling from G.I.C. Allahabad. As far as concerned my family, I belong to a middle-class family. My father is a Businessman, and my Mother is a homemaker. My brother is preparing for civil services. I am good in programming languages C, C++, and Java and very much interested in HTML, CSS, ASP. Net and SQL. My strength is self-confidence, positive attitude, hard work. My weakness is: I can easily believe every one. My hobbies are: Watching news channels, Playing volleyball, Listening to music.  !@#@!  Good morning/afternoon/evening  sir/mam, it's my pleasure to introduce myself. I am Anshika Bansal. I belong to Meerut. I have done my B.Tech in CSE from Lovely Professional University. I am carrying 5 years of experience at top Wall Street Companies. In my recent company, I led the development of an award-winning new trading platform. I can survive in a fast-paced environment. Now I am looking for a chance to apply my technical expertize and my creative problem-solving skills at an innovative software company like yours. "
What is Data Science?,"Data Science is the area of study which involves extracting insights from vast amounts of data using various scientific methods, algorithms, and processes. It helps you to discover hidden patterns from the raw data. The term Data Science has emerged because of the evolution of mathematical statistics, data analysis, and big data. !@#@! An interdisciplinary field that constitutes various scientific processes, algorithms, tools, and machine learning techniques working to help find common patterns and gather sensible insights from the given raw input data using statistical and mathematical analysis is called Data Science.The following figure represents the life cycle of data science.It starts with gathering the business requirements and relevant data.Once the data is acquired, it is maintained by performing data cleaning, data warehousing, data staging, and data architecture.Data processing does the task of exploring the data, mining it, analyzing it which can be finally used to generate the summary of the insights extracted from the data.Once the exploratory steps are completed, the cleansed data is subjected to various algorithms like predictive analysis, regression, text mining, recognition patterns, etc depending on the requirements.In the final stage, the results are communicated to the business in a visually appealing manner. This is where the skill of data visualization, reporting, and different business intelligence tools come into the picture."
What is the Difference Between Data Science and Machine Learning?,"Data Science is a combination of algorithms, tools, and machine learning technique which helps you to find common hidden patterns from the given raw data. Whereas Machine learning is a branch of computer science, that deals with system programming to automatically learn and improve with experience."
Name three types of biases that can occur during sampling,"In the sampling process, there are three types of biases, which are:Selection bias,Under coverage bias,Survivorship bias !@#@! Selection bias Undercoverage bias Survivorship bias"
Discuss Decision Tree algorithm,"A decision tree is a popular supervised machine learning algorithm. It is mainly used for Regression and Classification. It allows breaks down a dataset into smaller subsets. The decision tree can able to handle both categorical and numerical data. !@#@! Take the entire data set as input Calculate entropy of the target variable, as well as the predictor attributes Calculate your information gain of all attributes (we gain information on sorting different objects from each other) Choose the attribute with the highest information gain as the root node Repeat the same procedure on every branch until the decision node of each branch is finalized"
What is Prior probability and likelihood?,Prior probability is the proportion of the dependent variable in the data set while the likelihood is the probability of classifying a given observant in the presence of some other variable.
Explain Recommender Systems?,"It is a subclass of information filtering techniques. It helps you to predict the preferences or ratings which users likely to give to a product. !@#@! A recommender system predicts what a user would rate a specific product based on their preferences. It can be split into two different areas: Collaborative Filtering As an example, Last.fm recommends tracks that other users with similar interests play often. This is also commonly seen on Amazon after making a purchase; customers may notice the following message accompanied by product recommendations: ""Users who bought this also boughtâ€¦"" Content-based Filtering As an example: Pandora uses the properties of a song to recommend music with similar properties. Here, we look at content, instead of looking at who else is listening to music. !@#@! Recommender systems are a subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product."
Name three disadvantages of using a linear model,Three disadvantages of the linear model are:The assumption of linearity of the errors. You canâ€™t use this model for binary or count outcomes There are plenty of overfitting problems that it canâ€™t solve !@#@! The assumption of linearity of the errors It can't be used for count outcomes or binary outcomes There are overfitting problems that it can't solve 
Why do you need to perform resampling?,"Resampling is done in below-given cases:Estimating the accuracy of sample statistics by drawing randomly with replacement from a set of the data point or using as subsets of accessible data Substituting labels on data points when performing necessary tests Validating models by using random subsets  !@#@! Resampling is a methodology used to sample data for improving accuracy and quantify the uncertainty of population parameters. It is done to ensure the model is good enough by training the model on different patterns of a dataset to ensure variations are handled. It is also done in the cases where models need to be validated using random subsets or when substituting labels on data points while performing tests. !@#@! Resampling is done in any of these cases: Estimating the accuracy of sample statistics by using subsets of accessible data, or drawing randomly with replacement from a set of data points Substituting labels on data points when performing significance tests Validating models by using random subsets (bootstrapping, cross-validation)"
List out the libraries in Python used for Data Analysis and Scientific Computations.,SciPy Pandas Matplotlib NumPy SciKit Seaborn
 What is Power Analysis?,The power analysis is an integral part of the experimental design. It helps you to determine the sample size requires to find out the effect of a given size from a cause with a specific level of assurance. It also allows you to deploy a particular probability in a sample size constraint.
 Explain Collaborative filtering,"Collaborative filtering used to search for correct patterns by collaborating viewpoints, multiple data sources, and various agents. !@#@! Most recommender systems use this filtering process to find patterns and information by collaborating perspectives, numerous data sources, and several agents."
 What is bias?,Bias is an error introduced in your model because of the oversimplification of a machine learning algorithm.â€ It can lead to underfitting.
 Discuss Naive Bayes algorithm?,The Naive Bayes Algorithm model is based on the Bayes Theorem. It describes the probability of an event. It is based on prior knowledge of conditions which might be related to that specific event.
 What is a Linear Regression?,Linear regression is a statistical programming method where the score of a variable â€˜Aâ€™ is predicted from the score of a second variable â€˜Bâ€™. B is referred to as the predictor variable and A as the criterion variable. !@#@! Linear regression is a technique in which the score of a variable Y is predicted using the score of a predictor variable X. Y is called the criterion variable. Some of the drawbacks of Linear Regression are as follows:The assumption of linearity of errors is a major drawback.It cannot be used for binary outcomes. We have Logistic Regression for that.Overfitting problems are there that canâ€™t be solved.
 State the difference between the expected value and mean value,"They are not many differences, but both of these terms are used in different contexts. Mean value is generally referred to when you are discussing a probability distribution whereas expected value is referred to in the context of a random variable. !@#@! There are not many differences between these two, but it is to be noted that these are used in different contexts. The mean value generally refers to the probability distribution whereas the expected value is referred to in the contexts involving random variables."
 What the aim of conducting A/B Testing?,"AB testing used to conduct random experiments with two variables, A and B. The goal of this testing method is to find out changes to a web page to maximize or increase the outcome of a strategy. !@#@! This is statistical hypothesis testing for randomized experiments with two variables, A and B. The objective of A/B testing is to detect any changes to a web page to maximize or increase the outcome of a strategy."
 What is Ensemble Learning?,The ensemble is a method of combining a diverse set of learners together to improvise on the stability and predictive power of the model. Two types of Ensemble learning methods are: Bagging Bagging method helps you to implement similar learners on small sample populations. It helps you to make nearer predictions. Boosting Boosting is an iterative method which allows you to adjust the weight of an observation depends upon the last classification. Boosting decreases the bias error and helps you to build strong predictive models.
 Explain Eigenvalue and Eigenvector,"Eigenvectors are for understanding linear transformations. Data scientist need to calculate the eigenvectors for a covariance matrix or correlation. Eigenvalues are the directions along using specific linear transformation acts by compressing, flipping, or stretching. !@#@! Eigenvectors are column vectors or unit vectors whose length/magnitude is equal to 1. They are also called right vectors. Eigenvalues are coefficients that are applied on eigenvectors which give these vectors different values for length or magnitude.A matrix can be decomposed into Eigenvectors and Eigenvalues and this process is called Eigen decomposition. These are then eventually used in machine learning methods like PCA (Principal Component Analysis) for gathering valuable insights from the given matrix. !@#@! Eigenvalues are the directions along which a particular linear transformation acts by flipping, compressing, or stretching.Eigenvectors are for understanding linear transformations. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix. "
 Define the term cross-validation,"Cross-validation is a validation technique for evaluating how the outcomes of statistical analysis will generalize for an Independent dataset. This method is used in backgrounds where the objective is forecast, and one needs to estimate how accurately a model will accomplish. !@#@! Cross-validation is a model validation technique for evaluating how the outcomes of a statistical analysis will generalize to an independent data set. It is mainly used in backgrounds where the objective is to forecast and one wants to estimate how accurately a model will accomplish in practice. The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data set) to limit problems like overfitting and gain insight into how the model will generalize to an independent data set."
Explain the steps for a Data analytics project,The following are important steps involved in an analytics project: Understand the Business problem Explore the data and study it carefully.Prepare the data for modeling by finding missing values and transforming variables. Start running the model and analyze the Big data result. Validate the model with new data set. Implement the model and track the result to analyze the performance of the model for a specific period.
 Discuss Artificial Neural Networks,Artificial Neural networks (ANN) are a special set of algorithms that have revolutionized machine learning. It helps you to adapt according to changing input. So the network generates the best possible result without redesigning the output criteria.
 What is Back Propagation?,Back-propagation is the essence of neural net training. It is the method of tuning the weights of a neural net depend upon the error rate obtained in the previous epoch. Proper tuning of the helps you to reduce error rates and to make the model reliable by increasing its generalization.
 What is a Random Forest?,"Random forest is a machine learning method which helps you to perform all types of regression and classification tasks. It is also used for treating missing values and outlier values. !@#@! A random forest is built up of a number of decision trees. If you split the data into different packages and make a decision tree in each of the different groups of data, the random forest brings all those trees together. Steps to build a random forest model: Randomly select 'k' features from a total of 'm' features where k << m Among the 'k' features, calculate the node D using the best split point Split the node into daughter nodes using the best split Repeat steps two and three until leaf nodes are finalized Build forest by repeating steps one to four for 'n' times to create 'n' number of trees  !@#@! The underlying principle of this technique is that several weak learners combine to provide a strong learner. The steps involved are: Build several decision trees on bootstrapped training samples of data On each tree, each time a split is considered, a random sample of mm predictors is chosen as split candidates out of all pp predictors Rule of thumb: At each split m=pâˆšm=p Predictions: At the majority rule This exhaustive list is sure to strengthen your preparation for data science interview questions."
 What is the importance of having a selection bias?,"Selection Bias occurs when there is no specific randomization achieved while picking individuals or groups or data to be analyzed. It suggests that the given sample does not exactly represent the population which was intended to be analyzed. !@#@! The selection bias occurs in the case when the researcher has to make a decision on which participant to study. The selection bias is associated with those researches when the participant selection is not random. The selection bias is also called the selection effect. The selection bias is caused by as a result of the method of sample collection.Four types of selection bias are explained below:Sampling Bias: As a result of a population that is not random at all, some members of a population have fewer chances of getting included than others, resulting in a biased sample. This causes a systematic error known as sampling bias.Time interval: Trials may be stopped early if we reach any extreme value but if all variables are similar invariance, the variables with the highest variance have a higher chance of achieving the extreme value.Data: It is when specific data is selected arbitrarily and the generally agreed criteria are not followed.Attrition: Attrition in this context means the loss of the participants. It is the discounting of those subjects that did not complete the trial. !@#@! Selection bias, in general, is a problematic situation in which error is introduced due to a non-random population sample."
 What is the K-means clustering method?,K-means clustering is an important unsupervised learning method. It is the technique of classifying data using a certain set of clusters which is called K clusters. It is deployed for grouping to find out the similarity in the data.
 Explain the difference between Data Science and Data Analytics,"Data Scientists need to slice data to extract valuable insights that a data analyst can apply to real-world business scenarios. The main difference between the two is that the data scientists have more technical knowledge then business analyst. Moreover, they donâ€™t need an understanding of the business required for data visualization. !@#@! Data science involves the task of transforming data by using various technical analysis methods to extract meaningful insights using which a data analyst can apply to their business scenarios.Data analytics deals with checking the existing hypothesis and information and answers questions for a better and effective business-related decision-making process.Data Science drives innovation by answering questions that build connections and answers for futuristic problems. Data analytics focuses on getting present meaning from existing historical context whereas data science focuses on predictive modeling.Data Science can be considered as a broad subject that makes use of various mathematical and scientific tools and algorithms for solving complex problems whereas data analytics can be considered as a specific field dealing with specific concentrated problems using fewer tools of statistics and visualization."
 Explain p-value?,"When you conduct a hypothesis test in statistics, a p-value allows you to determine the strength of your results. It is a numerical number between 0 and 1. Based on the value it will help you to denote the strength of the specific result. !@#@! A p-value is the measure of the probability of having results equal to or more than the results achieved under a specific hypothesis assuming that the null hypothesis is correct. This represents the probability that the observed difference occurred randomly by chance.Low p-value which means values â‰¤ 0.05 means that the null hypothesis can be rejected and the data is unlikely with true null.High p-value, i.e values â‰¥ 0.05 indicates the strength in favor of the null hypothesis. It means that the data is like with true null.p-value = 0.05 means that the hypothesis can go either way. !@#@! p-value typically â‰¤ 0.05 This indicates strong evidence against the null hypothesis; so you reject the null hypothesis. p-value typically > 0.05 This indicates weak evidence against the null hypothesis, so you accept the null hypothesis. p-value at cutoff 0.05 This is considered to be marginal, meaning it could go either way."
 Define the term deep learning,"Deep Learning is a subtype of machine learning. It is concerned with algorithms inspired by the structure called artificial neural networks (ANN). !@#@! Deep learning is a paradigm of machine learning. In deep learning,  multiple layers of processing are involved in order to extract high features from the data. The neural networks are designed in such a way that they try to simulate the human brain. Deep learning has shown incredible performance in recent years because of the fact that it shows great analogy with the human brain.The difference between machine learning and deep learning is that deep learning is a paradigm or a part of machine learning that is inspired by the structure and functions of the human brain called the artificial neural networks. !@#@! Deep Learning is one of the essential factors in Data Science, including statistics. Deep Learning makes us work more closely with the human brain and reliable with human thoughts. The algorithms are sincerely created to resemble the human brain. In Deep Learning, multiple layers are formed from the raw input to extract the high-level layer with the best features."
 Explain the method to collect and analyze data to use social media to predict the weather condition.,"You can collect social media data using Facebook, twitter, Instagramâ€™s APIâ€™s. For example, for the tweeter, we can construct a feature from each tweet like tweeted date, retweets, list of follower, etc. Then you can use a multivariate time series model to predict the weather condition."
 When do you need to update the algorithm in Data science?,You need to update an algorithm in the following situation: You want your data model to evolve as data streams using  infrastructure The underlying data source is changingIf it is non-stationarity !@#@! You will want to update an algorithm when: You want the model to evolve as data streams through infrastructure The underlying data source is changing There is a case of non-stationarity 
 What is Normal Distribution,"A normal distribution is a set of a continuous variable spread across a normal curve or in the shape of a bell curve. You can consider it as a continuous probability distribution which is useful in statistics. It is useful to analyze the variables and their relationships when we are using the normal distribution curve. !@#@! Normal Distribution is also known as the Gaussian Distribution. The normal distribution shows the data near the mean and the frequency of that particular data. When represented in graphical form, normal distribution appears like a bell curve. The parameters included in the normal distribution are Mean, Standard Deviation, Median etc."
 Which language is best for text analytics? R or Python?,"Python will more suitable for text analytics as it consists of a rich library known as pandas. It allows you to use high-level data analysis tools and data structures, while R doesnâ€™t offer this feature."
 Explain the benefits of using statistics by Data Scientists,"Statistics help Data scientist to get a better idea of customerâ€™s expectation. Using the statistic method Data Scientists can get knowledge regarding consumer interest, behavior, engagement, retention, etc. It also helps you to build powerful data models to validate certain inferences and predictions."
 Name various types of Deep Learning Frameworks,Pytorch Microsoft Cognitive Toolkit TensorFlow Caffe Chainer Keras
Explain Auto-Encoder,Autoencoders are learning networks. It helps you to transform inputs into outputs with fewer numbers of errors. This means that you will get output to be as close to input as possible.
 Define Boltzmann Machine,Boltzmann machines is a simple learning algorithm. It helps you to discover those features that represent complex regularities in the training data. This algorithm allows you to optimize the weights and the quantity for the given problem.
 Explain why Data Cleansing is essential and which method you use to maintain clean data,"Dirty data often leads to the incorrect inside, which can damage the prospect of any organization. For example, if you want to run a targeted marketing campaign. However, our data incorrectly tell you that a specific product will be in-demand with your target audience; the campaign will fail."
 What is skewed Distribution & uniform distribution?,Skewed distribution occurs when if data is distributed on any one side of the plot whereas uniform distribution is identified when the data is spread is equal in the range.
 When underfitting occurs in a static model?,Underfitting occurs when a statistical model or machine learning algorithm not able to capture the underlying trend of the data.
 What is reinforcement learning?,"Reinforcement Learning is a learning mechanism about how to map situations to actions. The end result should help you to increase the binary reward signal. In this method, a learner is not told which action to take but instead must discover which action offers a maximum reward. As this method based on the reward/penalty mechanism."
 Name commonly used algorithms.,Four most commonly used algorithm by Data scientist are:  Linear regression Logistic regression Random Forest KNN
 What is precision?,"Precision is the most commonly used error metric is n classification mechanism. Its range is from 0 to 1, where 1 represents 100%"
 What is a univariate analysis?,"An analysis which is applied to none attribute at a time is known as univariate analysis. Boxplot is widely used, univariate model."
 How do you overcome challenges to your findings?,"In order, to overcome challenges of my finding one need to encourage discussion, Demonstrate leadership and respecting different options."
 Explain cluster sampling technique in Data science,"A cluster sampling method is used when it is challenging to study the target population spread across, and simple random sampling canâ€™t be applied."
State the difference between a Validation Set and a Test Set,A Validation set mostly considered as a part of the training set as it is used for parameter selection which helps you to avoid overfitting of the model being built. While a Test Set is used for testing or evaluating the performance of a trained machine learning model.
 Explain the term Binomial Probability Formula?,â€œThe binomial distribution contains the probabilities of every possible success on N trials for independent events that have a probability of Ï€ of occurring.â€
 What is a recall?,A recall is a ratio of the true positive rate against the actual positive rate. It ranges from 0 to 1.
 Discuss normal distribution,"Normal distribution equally distributed as such the mean, median and mode are equal."
" While working on a data set, how can you select important variables? Explain","Following methods of variable selection you can use: Remove the correlated variables before selecting important variables Use linear regression and select variables which depend on that p values. Use Backward, Forward Selection, and Stepwise Selection Use Xgboost, Random Forest, and plot variable importance chart. Measure information gain for the given set of features and select top n features accordingly."
 Is it possible to capture the correlation between continuous and categorical variable?,"Yes, we can use analysis of covariance technique to capture the association between continuous and categorical variables."
 Treating a categorical variable as a continuous variable would result in a better predictive model?,"Yes, the categorical value should be considered as a continuous variable only when the variable is ordinal in nature. So it is a better predictive model.These interview questions will also help in your viva(orals)"
What are some of the techniques used for sampling? What is the main advantage of sampling?,"Data analysis can not be done on a whole volume of data at a time especially when it involves larger datasets. It becomes crucial to take some data samples that can be used for representing the whole population and then perform analysis on it. While doing this, it is very much necessary to carefully take sample data out of the huge data that truly represents the entire dataset.There are majorly two categories of sampling techniques based on the usage of statistics, they are:Probability Sampling techniques: Clustered sampling, Simple random sampling, Stratified sampling.Non-Probability Sampling techniques: Quota sampling, Convenience sampling, snowball sampling, etc."
List down the conditions for Overfitting and Underfitting.,"Overfitting: The model performs well only for the sample training data. If any new data is given as input to the model, it fails to provide any result. These conditions occur due to low bias and high variance in the model. Decision trees are more prone to overfitting.Underfitting: Here, the model is so simple that it is not able to identify the correct relationship in the data, and hence it does not perform well even on the test data. This can happen due to high bias and low variance. Linear regression is more prone to Underfitting."
What do you understand by Imbalanced Data?,Data is said to be highly imbalanced if it is distributed unequally across different categories. These datasets result in an error in model performance and result in inaccuracy.
 What do you understand by Survivorship Bias?,This bias refers to the logical error while focusing on aspects that survived some process and overlooking those that did not work due to lack of prominence. This bias can lead to deriving wrong conclusions.
" Define the terms KPI, lift, model fitting, robustness and DOE.","KPI: KPI stands for Key Performance Indicator that measures how well the business achieves its objectives.Lift: This is a performance measure of the target model measured against a random choice model. Lift indicates how good the model is at prediction versus if there was no model.Model fitting: This indicates how well the model under consideration fits given observations.Robustness: This represents the systemâ€™s capability to handle differences and variances effectively.DOE: stands for the design of experiments, which represents the task design aiming to describe and explain information variation under hypothesized conditions to reflect variables."
 Define confounding variables.,Confounding variables are also known as confounders. These variables are a type of extraneous variables that influence both independent and dependent variables causing spurious association and mathematical relationships between those variables that are associated but are not casually related to each other.
 Define bias-variance trade-off?,"If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off."
 Define the confusion matrix?,"A confusion matrix is a table that is used to define the performance of a classification algorithm. The matrix is divided into two dimensions, that are predicted values and actual values along with the total number of predictions.Predicted values are those values, which are predicted by the model, and actual values are the true values for the given observations. Accuracy Misclassification rate Precision Recall F-score"
 What is logistic regression? ,Logistic Regression is also known as the logit model. It is a technique to predict the binary outcome from a linear combination of variables (called the predictor variables). 
 What is a Gradient and Gradient Descent?,"Gradient: Gradient is the measure of a property that how much the output has changed with respect to a little change in the input. In other words, we can say that it is a measure of change in the weights with respect to the change in error. The gradient can be mathematically represented as the slope of a function.Gradient Descent: Gradient descent is a minimization algorithm that minimizes the Activation function. Well, it can minimize any function given to it but it is usually provided with the activation function only. Gradient descent, as the name suggests means descent or a decrease in something. "
How is logistic regression done?,Logistic regression measures the relationship between the dependent variable (our label of what we want to predict) and one or more independent variables (our features) by estimating probability using its underlying logistic function (sigmoid).
How can you avoid overfitting your model?,"Overfitting refers to a model that is only set for a very small amount of data and ignores the bigger picture. There are three main methods to avoid overfitting: Keep the model simpleâ€”take fewer variables into account, thereby removing some of the noise in the training data Use cross-validation techniques, such as k folds cross-validation Use regularization techniques, such as LASSO, that penalize certain model parameters if they're likely to cause overfitting "
"Differentiate between univariate, bivariate, and multivariate analysis.","Univariate Univariate data contains only one variable. The purpose of the univariate analysis is to describe the data and find patterns that exist within it. Bivariate Bivariate data involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to determine the relationship between the two variables.Multivariate Multivariate data involves three or more variables, it is categorized under multivariate. It is similar to a bivariate but contains more than one dependent variable."
What are the feature selection methods used to select the right variables?,"There are two main methods for feature selection, i.e, filter, and wrapper methods. Filter Methods This involves: Linear discrimination analysis ANOVA Chi-Square The best analogy for selecting features is ""bad data in, bad answer out."" When we're limiting or selecting the features, it's all about cleaning up the data coming in.Wrapper Methods This involves: Forward Selection: We test one feature at a time and keep adding them until we get a good fit Backward Selection: We test all the features and start removing them to see what works better Recursive Feature Elimination: Recursively looks through all the different features and how they pair together Wrapper methods are very labor-intensive, and high-end computers are needed if a lot of data analysis is performed with the wrapper method. "
You are given a data set consisting of variables with more than 30 percent missing values. How will you deal with them?,"The following are ways to handle missing data values: If the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; we use the rest of the data to predict the values.For smaller data sets, we can substitute missing values with the mean or average of the rest of the data using the pandas' data frame in python. There are different ways to do so, such as df.mean(), df.fillna(mean)."
 What are dimensionality reduction and its benefits?,"The Dimensionality reduction refers to the process of converting a data set with vast dimensions into data with fewer dimensions (fields) to convey similar information concisely. This reduction helps in compressing data and reducing storage space. It also reduces computation time as fewer dimensions lead to less computing. It removes redundant features; for example, there's no point in storing a value in two different units (meters and inches)"
 How should you maintain a deployed model?,"The steps to maintain a deployed model are: Monitor Constant monitoring of all models is needed to determine their performance accuracy. When you change something, you want to figure out how your changes are going to affect things. This needs to be monitored to ensure it's doing what it's supposed to do.Evaluate Evaluation metrics of the current model are calculated to determine if a new algorithm is needed. Compare The new models are compared to each other to determine which model performs the best. build The best-performing model is re-built on the current state of data."
 How can you select k for k-means? ,"We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k-means clustering on the data set where 'k' is the number of clusters. Within the sum of squares (WSS), it is defined as the sum of the squared distance between each member of the cluster and its centroid "
 How can outlier values be treated?,"You can drop outliers only if it is a garbage value. If the outliers have extreme values, they can be removed.If you cannot drop outliers, you can try the following:Try a different model. Data detected as outliers by linear models can be fit by nonlinear models. Therefore, be sure you are choosing the correct model.Try normalizing the data. This way, the extreme data points are pulled to a similar range.You can use algorithms that are less affected by outliers; an example would be random forests. "
 How can time-series data be declared as stationery?,It is stationary when the variance and mean of the series are constant with time.
 How can you calculate accuracy using a confusion matrix?,The formula for accuracy is: Accuracy = (True Positive + True Negative) / Total Observations
 'People who bought this also boughtâ€¦' recommendations seen on Amazon are a result of which algorithm?,"The recommendation engine is accomplished with collaborative filtering. Collaborative filtering explains the behavior of other users and their purchase history in terms of ratings, selection, etc. The engine makes predictions on what might interest a person based on the preferences of other users. In this algorithm, item features are unknown."
 You are given a dataset on cancer detection. You have built a classification model and achieved an accuracy of 96 percent. Why shouldn't you be happy with your model performance? What can you do about it?,"Cancer detection results in imbalanced data. In an imbalanced dataset, accuracy should not be based as a measure of performance. It is important to focus on the remaining four percent, which represents the patients who were wrongly diagnosed. Early diagnosis is crucial when it comes to cancer detection, and can greatly improve a patient's prognosis.Hence, to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine the class wise performance of the classifier."
" Which of the following machine learning algorithms can be used for inputting missing values of both categorical and continuous variables? K-means clustering,Linear regression,K-NN (k-nearest neighbor),Decision trees ","The K nearest neighbor algorithm can be used because it can compute the nearest neighbor and if it doesn't have a value, it just computes the nearest neighbor based on all the other features. When you're dealing with K-means clustering or linear regression, you need to do that in your pre-processing, otherwise, they'll crash. Decision trees also have the same problem, although there is some variance."
 What do you understand about true positive rate and false-positive rate?,The True Positive Rate (TPR) defines the probability that an actual positive will turn out to be positive. The True Positive Rate (TPR) is calculated by taking the ratio of the [True Positives (TP)] and [True Positive (TP) & False Negatives (FN) ]. The formula for the same is stated below - TPR=TP/TP+FN The False Positive Rate (FPR) defines the probability that an actual negative result will be shown as a positive one i.e the probability that a model will generate a false alarm. The False Positive Rate (FPR) is calculated by taking the ratio of the [False Positives (FP)] and [True Positives (TP) & False Positives(FP)].The formula for the same is stated below -FPR=FP/TN+FP
 What is the ROC curve?,"The graph between the True Positive Rate on the y-axis and the False Positive Rate on the x-axis is called the ROC curve and is used in binary classification.The False Positive Rate (FPR) is calculated by taking the ratio between False Positives and the total number of negative samples, and the True Positive Rate (TPR) is calculated by taking the ratio between True Positives and the total number of positive samples."
 What is a Confusion Matrix?,The Confusion Matrix is the summary of prediction results of a particular problem. It is a table that is used to describe the performance of the model. The Confusion Matrix is an n*n matrix that evaluates the performance of the classification model.
 What do you understand about the true-positive rate and false-positive rate?,TRUE-POSITIVE RATE: The true-positive rate gives the proportion of correct predictions of the positive class. It is also used to measure the percentage of actual positives that are accurately verified. FALSE-POSITIVE RATE: The false-positive rate gives the proportion of incorrect predictions of the positive class. A false positive determines something is true when that is initially false.
 How is Data Science different from traditional application programming?,"The primary and vital difference between Data Science and traditional application programming is that in traditional programming, one has to create rules to translate the input to output. In Data Science, the rules are automatically produced from the data."
 What is the difference between the long format data and wide format data?,"LONG FORMAT DATA: It contains values that repeat in the first column. In this format, each row is a one-time point per subject. WIDE FORMAT DATA: In the Wide Format Data, the dataâ€™s repeated responses will be in a single row, and each response can be recorded in separate columns."
 Mention some techniques used for sampling. What is the main advantage of sampling?,"Sampling is the selection of individual members or a subset of the population to estimate the characters of the whole population. There are two types of Sampling, namely Probability and Non-Probability Sampling."
 Why is Python used for Data Cleaning in DS?,"Data Scientists and technical analysts must convert a huge amount of data into effective ones. Data Cleaning includes removing malwared records, outliners, inconsistent values, redundant formatting etc. Matplotlib, Pandas etc are the most used Python Data Cleaners."
 What are the popular libraries used in Data Science?,The popular libraries used in Data Science are Tensor Flow Pandas NumPy SciPy Scrapy Librosa MatPlotLib 
 What is variance in Data Science?,Variance is the value that depicts the individual figures in a set of data which distributes themselves about the mean and describes the difference of each value from the mean value. Data Scientists use variance to understand the distribution of a data set.
 What is pruning in a decision tree algorithm?,"In Data Science and Machine Learning, Pruning is a technique which is related to decision trees. Pruning simplifies the decision tree by reducing the rules. Pruning helps to avoid complexity and improves accuracy. Reduced error Pruning, cost complexity pruning etc. are the different types of Pruning."
 What is entropy in a decision tree algorithm?,"Entropy is the measure of randomness or disorder in the group of observations. It also determines how a decision tree switches to split data. Entropy is also used to check the homogeneity of the given data. If the entropy is zero, then the sample of data is entirely homogeneous, and if the entropy is one, then it indicates that the sample is equally divided."
 What information is gained in a decision tree algorithm?,Information gain is the expected reduction in entropy. Information gain decides the building of the tree. Information Gain makes the decision tree smarter. Information gain includes parent node R and a set E of K training examples. It calculates the difference between entropy before and after the split.
 What is k-fold cross-validation?,"The k-fold cross validation is a procedure used to estimate the model's skill in new data. In k-fold cross validation, every observation from the original dataset may appear in the training and testing set. K-fold cross-validation estimates the accuracy but does not help you to improve the accuracy."
 What is an RNN (recurrent neural network)?,"RNN is an algorithm that uses sequential data. RNN is used in language translation, voice recognition, image capturing etc. There are different types of RNN networks such as one-to-one, one-to-many, many-to-one and many-to-many. RNN is used in Googleâ€™s Voice search and Appleâ€™s Siri."
 What are the feature vectors?,"A feature vector is an n-dimensional vector of numerical features that represent an object. In machine learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an object in a mathematical way that's easy to analyze."
 What are the steps in making a decision tree?,Take the entire data set as input.Look for a split that maximizes the separation of the classes. A split is any test that divides the data into two sets.Apply the split to the input data (divide step).Re-apply steps one and two to the divided data.Stop when you meet any stopping criteria.This step is called pruning. Clean up the tree if you went too far doing splits.
 What is root cause analysis?,Root cause analysis was initially developed to analyze industrial accidents but is now widely used in other areas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor is called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event from recurring.
 What is logistic regression?,Logistic regression is also known as the logit model. It is a technique used to forecast the binary outcome from a linear combination of predictor variables.
 Do gradient descent methods always converge to similar points?,"They do not, because in some cases, they reach a local minima or a local optima point. You would not reach the global optima point. This is governed by the data and the starting conditions."
 What is the law of large numbers?,"It is a theorem that describes the result of performing the same experiment very frequently. This theorem forms the basis of frequency-style thinking. It states that the sample mean, sample variance, and sample standard deviation converge to what they are trying to estimate."
  What are the confounding variables?,These are extraneous variables in a statistical model that correlates directly or inversely with both the dependent and the independent variable. The estimate fails to account for the confounding factor.
 What is star schema?,"It is a traditional database schema with a central table. Satellite tables map IDs to physical names or descriptions and can be connected to the central fact table using the ID fields; these tables are known as lookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes, star schemas involve several layers of summarization to recover information faster."
 What is survivorship bias?,Survivorship bias is the logical error of focusing on aspects that support surviving a process and casually overlooking those that did not because of their lack of prominence. This can lead to wrong conclusions in numerous ways.
 What is a bias-variance trade-off?,"While trying to get over bias in our model, we try to increase the complexity of the machine learning algorithm. Though it helps in reducing the bias, after a certain point, it generates an overfitting effect on the model hence resulting in hyper-sensitivity and high variance. Bias-Variance trade-off: To achieve the best performance, the main target of a supervised machine learning algorithm is to have low variance and bias. "
 Describe Markov chains?,"Markov Chains defines that a stateâ€™s future probability depends only on its current state. Markov chains belong to the Stochastic process type category.A perfect example of the Markov Chains is the system of word recommendation. In this system, the model recognizes and recommends the next word based on the immediately previous word and not anything before that. The Markov Chains take the previous paragraphs that were similar to training data-sets and generates the recommendations for the current paragraphs accordingly based on the previous word."
 Why is R used in Data Visualization?,"R is widely used in Data Visualizations for the following reasons- We can create almost any type of graph using R. R has multiple libraries like lattice, ggplot2, leaflet, etc., and so many inbuilt functions as well.  It is easier to customize graphics in R compared to Python. R is used in feature engineering and in exploratory data analysis as well."
 What is the difference between a box plot and a histogram?,"The frequency of a certain featureâ€™s values is denoted visually by both box plots and histograms. Boxplots are more often used in comparing several datasets and compared to histograms, take less space and contain fewer details. Histograms are used to know and understand the probability distribution underlying a dataset."
 What does NLP stand for?,"NLP is short for Natural Language Processing. It deals with the study of how computers learn a massive amount of textual data through programming. A few popular examples of NLP are Stemming, Sentimental Analysis, Tokenization, removal of stop words, etc."
 Difference between Normalisation and Standardization,"Standardization is the technique of converting data in such a way that it is normally distributed and has a standard deviation of 1 and a mean of 0.Standardization takes care that the standard normal distribution is followed by the data. Normalization is the technique of converting all data values to lie between 1 and 0 . This is also known as min-max scaling. The data returning into the 0 to 1 range is taken care of by Normalization.
"
 Difference between Point Estimates and Confidence Interval,"Confidence Interval: A range of values likely containing the population parameter is given by the confidence interval. Further, it even tells us how likely that particular interval can contain the population parameter. The Confidence Coefficient (or Confidence level) is denoted by 1-alpha, which gives the probability or likeness. The level of significance is given by alpha. Point Estimates: An estimate of the population parameter is given by a particular value called the point estimate. Some popular methods used to derive Population Parametersâ€™ Point estimators are - Maximum Likelihood estimator and the Method of Moments."
What is Python?,Python is a high-level and object-oriented programming language with unified semantics designed primarily for developing apps and the web. It is the core language in the field of Rapid Application Development (RAD) as it offers options such as dynamic binding and dynamic typing.
What are the benefits of Python?,"The benefits of Python are as follows: Speed and Productivity: Utilizing the productivity and speed of Python will enhance the process control capabilities and possesses strong integration. Extensive Support for Libraries: Python provides a large standard library that includes areas such as operating system interfaces, web service tools, internet protocols, and string protocols. Most of the programming tasks are already been scripted in the standard library which reduces effort and time. User-friendly Data Structures: Python has an in-built dictionary of data structures that are used to build fast user-friendly data structures. Existence of Third-Party Modules: The presence of third-party modules in the Python Package Index (PyPI) will make Python capable to interact with other platforms and languages. Easy Learning: Python provides excellent readability and simple syntaxes to make it easy for beginners to learn."
What are the key features of Python? ,"The following are the significant features of Python, and they are: Interpreted Language: Python is an interpreted language that is used to execute the code line by line at a time. This makes debugging easy. Highly Portable: Python can run on different platforms such as Unix, Macintosh, Linux, Windows, and so on. So, we can say that it is a highly portable language. Extensible: It ensures that the Python code can be compiled on various other languages such as C, C++, and so on. GUI programming Support: It implies that Python provides support to develop graphical user interfaces."
What type of language is Python? Programming or Scripting? ,"Python is suitable for scripting, but in general, it is considered a general-purpose programming language."
What are the applications of Python?,The applications of Python are as follows: GUI-based desktop applications Image processing applications Business and Enterprise applications Prototyping Web and web framework applications. Local Variables in Python: The variables that are declared inside a function are called local variables. These types of variables can be accessed only inside the function.
What is the difference between a list and a tuple in Python?,`The difference between a tuple and list is as follows: List Tuple The list is mutable (can be changed) A tuple is immutable (remains constant) These lists performance is slower Tuple performance is faster when compared to lists 
What are the global and local variables in Python?,Global Variables in Python: The variables that are declared outside the function are called global variables. These variables can be accessed or invoked by any function in the program.
Define PYTHON PATH?,"PYTHONPATH is an environmental variable that is used when we import a module. Suppose at any time we import a module, PYTHONPATH is used to check the presence of the modules that are imported in different directories. Loading of the module will be determined by interpreters."
What are the two major loop statements?,for and while
Why do we use the split method in Python?,split() method in Python is mainly used to separate a given string.
How memory management is done in Python?,In Python memory management is done using private heap space. The private heap is the storage area for all the data structures and objects. The interpreter has access to the private heap and the programmer cannot access this private heap.The storage allocation for the data structures and objects in Python is done by the memory manager. The access for some tools is provided by core API for programmers to code. The built-in garbage collector in Python is used to recycle all the unused memory so that it can be available for heap storage area.
How do we reverse a list in Python?,By using the list.reverse(): we can reverse the objects of the list in Python.
Define modules in Python?,The module is defined as a file that includes a set of various functions and Python statements that we want to add to our application.
What are the built-in types available in Python?,The built-in types in Python are as follows: Integer Complex numbers Floating-point numbers Strings Built-in functions.
What are Python Decorators? ,Decorator is the most useful tool in Python as it allows programmers to alter the changes in the behavior of class or function. 
How do we find bugs and statistical problems in Python?,"We can detect bugs in python source code using a static analysis tool named PyChecker. Moreover, there is another tool called PyLint that checks whether the Python modules meet their coding standards or not."
What is the difference between .py and .pyc files?,py files are Python source files. .pyc files are the compiled bytecode files that are generated by the Python compiler.
How do you invoke the Python interpreter for interactive use?,By using python or pythonx. y we can invoke a Python interpreter. where x.y is the version of the Python interpreter.
Define String in Python?,String in Python is formed using a sequence of characters. Value once assigned to a string cannot be modified because they are immutable objects. String literals in Python can be declared using double quotes or single quotes.
What do you understand by the term namespace in Python?, A namespace in Python can be defined as a system that is designed to provide a unique name for every object in python. Types of namespaces that are present in Python are: Local namespace Global namespace Built-in namespace Scope of an object in Python: Scope refers to the availability and accessibility of an object in the coding region.
How do you create a Python function?,Functions are defined using the def statement.
Define iterators in Python?,"In Python, an iterator can be defined as an object that can be iterated or traversed upon. In another way, it is mainly used to iterate a group of containers, elements, the same as a list."
How does a function return values?,Functions return values using the return statement.
Define slicing in Python?,"Slicing is a procedure used to select a particular range of items from sequence types such as Strings, lists, and so on."
How can Python be an interpreted language?,"As in Python the code which we write is not machine-level code before runtime so, this is the reason why Python is called an interpreted language. "
What happens when a function doesn’t have a return statement? Is this valid?,"Yes, this is valid. The function will then return a None object. The end of a function is defined by the block of code that is executed (i.e., the indenting) not by any explicit keyword."
Define package in Python?,In Python packages are defined as the collection of different modules.
How can we make a Python script executable on Unix?,"In order to make a Python script executable on Unix, we need to perform two things. They are: Script file mode must be executable and The first line should always begin with #."
Which command is used to delete files in Python?,OS.unlink(filename) or OS.remove(filename) are the commands used to delete files in Python Programming.
Define pickling and unpickling in Python?,Pickling in Python: The process in which the pickle module accepts various Python objects and converts them into a string representation and dumps the file accordingly using the dump function is called pickling. Unpickling in Python: The process of retrieving actual Python objects from the stored string representation is called unpickling.
Explain the difference between local and global namespaces?,Local namespaces are created within a function when that function is called. Global namespaces are created when the program starts.
What is a boolean in Python?,"Boolean is one of the built-in data types in Python, it mainly contains two values, which are true and false. Python bool() is the method used to convert a value to a boolean value."
What are Python String formats and Python String replacements?,Python String Format: The String format() method in Python is mainly used to format the given string into an accurate output or result. Python String Replace: This method is mainly used to return a copy of the string in which all the occurrence of the substring is replaced by another substring.
Name some of the built-in modules in Python?,The built-in modules in Python are: sys module OS module random module collection module JSON Math module.
What are the functions in Python?,"In Python, functions are defined as a block of code that is executable only when it is called. The def keyword is used to define a function in Python."
What are Dict and List comprehensions in Python?,These are mostly used as syntax constructions to ease the creation of lists and dictionaries based on existing iterable.
Define the term lambda?,Lambda is the small anonymous function in Python that is often used as an inline function.
When would you use triple quotes as a delimiter?,"Triple quotes ‘’” or ‘“ are string delimiters that can span multiple lines in Python. Triple quotes are usually used when spanning multiple lines, or enclosing a string that has a mix of single and double quotes contained therein."
Define self in Python?,"In Python self is defined as an object or an instance of a class. This self is explicitly considered as the first parameter in Python. Moreover, we can also access all the methods and attributes of the classes in Python programming using self keyword. In the case of the init method, self refers to the newer creation of the object. Whereas in the case of other methods self refers to the object whose method was called."
What is _init_?,The _init_ is a special type of method in Python that is called automatically when the memory is allocated for a new object. The main role of _init_ is to initialize the values of instance members for objects. 
Define generators in Python?,The way of implementing an effective representation of iterators is known as generators. It is only the normal function that yields expression in the function.
Define docstring in Python?,"The docstring in Python is also called a documentation string, it provides a way to document the Python classes, functions, and modules."
How do we convert the string to lowercase?,the lower() function is used to convert string to lowercase.
How to remove values from a Python array?,The elements can be removed from a Python array using the remove() or pop() function.
What is Try Block?,A block that is preceded by the try keyword is known as a try block.
How can we access a module written in Python from C?,"We can access the module written in Python from C by using the following method. Module == PyImport_ImportModule(""<modulename>"");"
How do you copy an object in Python?,To copy objects in Python we can use methods called copy.copy() or copy.deepcopy().
"What do you understand by database, and what does it have?","A database can be defined as the structured form of data storage from which data can be retrieved and managed based on requirements. Basically, a database consists of tables where data is stored in an organized manner. Each table consists of rows and columns to store data. Data can be stored, modified, updated, and accessed easily in a database. For instance, a bank management database or school management database are a few examples of databases."
What are DBMS and RDBMS?,"DBMS – Database Management System. DBMS is the software that allows storing, modifying, and retrieving data from a database. And it is a group of programs that act as the interface between data and applications. DBMS supports receiving queries from applications and retrieving data from the database. RDBMS – Relational Database Management System Like DBMS, RDBMS is also the software that allows storing, modifying, and retrieving data from a database but a RELATIONAL database. In a relational database, the data in the tables have a relationship. Besides, RDBMS is useful when data in tables are being managed securely and consistently."
What are Query and Query language?,"A query is nothing but a request sent to a database to retrieve data or information. The required data can be retrieved from a table or many tables in the database. Query languages use various types of queries to retrieve data from databases. SQL, Datalog, and AQL are a few examples of query languages; however, SQL is known to be the widely used query language. SQL returns data as columns and rows in a table, whereas other languages return data in other forms, like graphs, charts, etc."
What do you mean by subquery?,"It is a query that exists inside the statements such as SELECT, INSERT, UPDATE, and DELETE. It may exist inside a subquery too. A subquery is also known as an inner query or inner select. The statement with a subquery is an outer query or outer select."
"What is SQL, and mention its uses?","SQL – Structured Query Language. SQL is known as the query programming language. It uses SQL queries to store, modify and retrieve data into and from databases. Briefly, SQL inserts, updates, and deletes data in databases; creates new databases and new tables; creates views and stored procedures; and sets permissions on the database objects."
Can you brief the DML Commands?,INSERT This command allows inserting a data into a table of a database DELETE This command allows deleting specific rows from a table UPDATE This command allows modifying a data in a table.
What do you understand by tables and fields in a database?,"Tables are the database objects where data is stored logically. Like a spreadsheet, data is stored in the form of rows and columns in a database table. A row in a table represents a record, and columns represent the different fields. Fields have the data types such as text, dates, numbers, and links. For example, consider the below customer database in which rows consist of the company names and columns consist of the various details of customers like first name, last name, age, location, etc. Here, number 1 indicates a record, number 2 indicates a field, and number 3 indicates the field value."
What are the different types of tables used in SQL?,The following are the table types used in SQL: Partitioned tables Temporary tables System tables Wide tables.
What are temporary tables?,"Temporary tables only store data during the current session, and they will be dropped once the session is over. With temporary tables, you can create, read, update and delete records like permanent tables. Know that there are two types of temporary tables: local and global temporary tables. Local temporary tables are only visible to the user who created them, and they are deleted the moment the user disconnects from the instance of the SQL server. On the contrary, global temporary tables are visible to all users, and they are deleted only when all the users who reference the tables get disconnected."
What do you mean by Primary Key and Foreign Key in SQL?,"Primary Key: A primary is a field or combination of many fields that help identify records in a table. Note that there can be only one primary key for a table. The table that has the primary key is known as the parent table. Foreign Key: A foreign key is the field or combination of fields of a table that links the primary key of another table. A foreign key is used to create a connection between two tables. Unlike a primary key, a table can have one or many foreign keys. The table that has a foreign key is known as the child table. For example, customer ID (1) is the primary key of the Customers table, and customer ID (2) in the orders table is identified as the foreign key to the customer's table."
What are Superkey and candidate key?,"A super key may be a single or a combination of keys that help to identify a record in a table. Know that Super keys can have one or more attributes, even though all the attributes are not necessary to identify the records. A candidate key is the subset of Superkey, which can have one or more than one attributes to identify records in a table. Unlike Superkey, all the attributes of the candidate key must be helpful to identify the records. Note that all the candidate keys can be Super keys, but all the super keys cannot be candidate keys."
What are composite keys?,"A composite key is the combination of two or more columns in a table used to identify a row in a table. Know that a combination of columns is essential in creating composite keys because a single column in a composite key cannot identify a row in a table. We can say that the composite key is the primary key with a few more attributes or columns. Also, a composite key can be a combination of candidate keys."
"What is JOIN operation in SQL, and mention their types?","JOIN is the logical operation used to retrieve data from two or more tables. It can be applied only when there is a logical relationship between two tables. Moreover, the JOIN operator uses the data of one table to retrieve data from another table. Following are the different types of logical operations: INNER JOIN LEFT (OUTER) JOIN RIGHT (OUTER) JOIN FULL (OUTER) JOIN CROSS JOIN"
What do you mean by Self Join?,"In self-join operation, a table is joined with itself to retrieve the desired data. Every join operation needs two tables as a basic rule. Therefore, in self-join, a table is joined with an instance of the same table. By doing this, values of the two table columns are compared with each other, and the desired data is retrieved as the result set."
What do you mean by Cross Join?,"Cross Join is basically the Cartesian product type in which each row in a table is paired with all the rows of another table. So, the result set will be the paired combinations of the rows of two tables. Generally, cross join is not preferred by developers as it increases complexity in programs when there are many rows in tables. But, it can be used in queries if you identify normal join operation won’t be effective for your query."
What are the SQL constraints?,SQL constraints specify conditions for a column or table to manage the data stored in tables effectively. The following are the commonly used SQL constraints. NOT NULL - This condition ensures columns won’t accept a NULL value. UNIQUE - It ensures that all the values in a column must be unique. CHECK - It ensures that all the column fields obey a specific condition. DEFAULT - It provides a default value for the fields of a column unless no value is specified for the fields CREATE INDEX - It ensures creating an index for tables so that retrieving data from the tables becomes easier PRIMARY KEY - It must identify every row of a table FOREIGN KEY -  It must link tables based on common attributes.
What are local and global variables?,"Local variables are declared inside a function so that only that function can call them. They only exist until the execution of that specific function. Generally, local variables are stored in stack memory and cleaned up automatically. Global variables are declared outside of a function. They are available until the execution of the entire program. Unlike local variables, global variables are stored in fixed memory and not cleaned up automatically."
"What is an index in SQL, and mention its types?","An index is used to retrieve data from a database quickly. Generally, indexes have keys taken from the columns of tables and views. We can say, SQL indexes are similar to the indexes in books that help to identify pages in the books quickly. There are two types of indexes: Clustered indexes Non-clustered indexes."
Mention the different types of SQL commands or SQL subsets?,There are five types of SQL commands offered in SQL. They are given as follows; DDL - Data Definition Languages DML - Data Manipulation Languages DCL - Data Control Language TCL - Transaction Control Language DQL - Data Query Language.
What are the Various Commands used in SQL Subsets?,"DDL-CREATE, DROP, ALTER, TRUNCATE, ADD COLUMN, and DROP COLUMN. DML-INSERT, DELETE, and UPDATE. DCL-GRANT and REVOKE.TCL-COMMIT, ROLLBACK, SAVEPOINT, and SET TRANSACTION.DQL-SELECT."
Can you brief me on a few DDL Commands?,"ALTER This command allows changing the structure of a table CREATE It allows the creation of database objects such as tables, views, and indexes. DROP This command allows removing database objects from a database TRUNCATE This command helps to delete all the rows of a table permanently. "
Can you brief me on a few DCL Commands?,GRANT This command can be used to share a database with other users. All the database objects can be granted access with certain rights to users. REVOKE This command can be applied if you want to restrict the access of database objects by other users.
Can you brief me about TCL commands?,COMMIT This command allows for saving the transactions made in a database. ROLLBACK This command helps undo the transactions made in a database with the condition that the transactions shouldn't be saved yet. SAVEPOINT This command helps to roll the transactions up to a certain point but not the entire transaction.
What are Stored Procedures?,"It is a function that consists of a group of statements that can be stored and executed whenever it is required. Know that stored procedures are compiled only once. They are stored as ‘Named Object’ in the SQL server database. Stored procedures can be called at any time during program execution. Moreover, a stored procedure can be called another stored procedure."
What are the SQL database functions?,"SQL offers the flexibility to developers to use built-in functions as well as user-defined functions. The functions are categorized as follows: Aggregate functions: They process a group of values and return a single value. They can combine with GROUP BY, OVER, HAVING clauses and return values. They are deterministic functions. Analytic functions: They are similar to aggregate functions but return multiple rows as result set after processing a group of values. They help calculate moving averages, running totals, Top-N results, percentages, etc. Ranking functions: They return ranking values for rows in a table based on the given conditions. Here, the results are non-deterministic. Rowset functions: They return an object used as the table reference. Scalar functions: They operate on a single value and return a single value."
Mention the different types of operators used in SQL?,"There are six types of operators used in SQL. They are given as follows: Arithmetic Operators Addition, Subtraction, Multiplication, Division, and Remainder/Modulus Bitwise Operators Bitwise AND, Bitwise OR, Bitwise XOR, etc. Comparison Operators Equal to, Not equal to, Greater than, Not greater than, Less than, Not less than, Not equal to, etc. Compound Operators Add equals, Multiply equals, Subtract equals, Divide equals, and Modulo equals Logical Operators ALL, ANY/SOME, AND, BETWEEN, NOT, EXISTS, OR, IN, LIKE, and ISNULL String Operators String concatenation, wildcard, character matches, etc."
What are the Set Operators?,There are four types of set operators available in SQL. They are given as follows: Union This operator allows combining result sets of two or more SELECT statements. Union All This operator allows combining result sets of two or more SELECT statements along with duplicates. Intersect This operator returns the common records of the result sets of two or more SELECT statements. Minus This operator returns the exclusive records of the first table when two tables undergo this operation.
How would you differentiate single-row functions from multiple-row functions?,Single row functions can act on a single row of a table at a time. They return only one result after executing a row. Length and case conversions are known to be single-row functions. Multiple row functions can act on multiple rows of a table at a time. They are also called group functions and return a single output after executing multiple rows.
What are Tuple and tuple functions?,"A tuple is a single row in a table that represents a single record of a relation. A tuple contains all the data that belongs to a record. At the same time, tuple functions allow retrieving tuples from a database table. They are extensively used in analysis services that have multidimensional structures. For example, the highlighted row in the below table shows all the data belonging to a customer, which is nothing but a tuple."
What do you mean by dependency and mention the different dependencies?,Dependency is the relation between the attributes of a table. The following are the different types of dependencies in SQL. Functional dependency Fully-functional dependency Multivalued dependency Transitive dependency Partial dependency.
What do you mean by Data Integrity?,"Data integrity ensures the accuracy and consistency of data stored in a database. Data integrity, in a way, represents the data quality. So, the data characteristics defined for a column should be satisfied while storing data in the columns. For instance, if a column in a table is supposed to store numeric values, then it should not accept Alphabetic values; otherwise, you can mean that data integrity is lost in the table."
What is Database Cardinality?,"Database Cardinality denotes the uniqueness of values in the tables. It supports optimizing query plans and hence improves query performance. There are three types of database cardinalities in SQL, as given below: Higher Cardinality Normal Cardinality  Lower Cardinality."
What are database Normalisation and various forms of Normalisation?,It is the process that reduces data redundancy and improves data integrity by restructuring the relational database. The following are the different forms of normalization: First normal form – 1NF Second normal form – 2 NF Third normal form – 3 NF Boyce Codd Normal Form/Fourth Normal form – BCNF/4NF.
"What is Cursor, and how to use it?","In general, the result set of a SQL statement is a set of rows. If we need to manipulate the result set, we can act on a single row of the result set at a time. Cursors are the extensions to the result set and help point a row in the result set. Here, the pointed row is known as the current row. Cursors can be used in the following ways: Positions a row in a result set Supports retrieving the current row from the result set Supports data modifications in the current row Allowing  SQL statements in stored procedures, scripts, and triggers to access the result set."
Mention the different types of Cursors?,"Forward Only: It is known as the firehose cursor that can make only a forward movement. The modification made by the current user and other users is visible while using this cursor. As it is the forward-moving cursor, it fetches rows of the result set from the start to end serially. Static: This cursor can move forward and backward on the result set. Here, only the same result set is visible throughout the lifetime of the cursor. In other words, once the cursor is open, it doesn’t show any changes made in the database that is the source for the result set. Keyset: This cursor is managed by a set of identifiers known as keys or keysets. Here, the keysets are built by the columns that derive the rows of a result set. When we use this cursor, we can’t view the records created by other users. Similarly, if any user deletes a record, we can’t access that record too. Dynamic: Unlike static cursors, once the cursor is open, all the modifications performed in the database are reflected in the result set. The UPDATE, INSERT and DELETE operations made by other users can be viewed while scrolling the cursor."
What are Entity and Relationship?,"Entities are real-world objects that are individualistic and independent. Rows of a table represent the members of the entity, and columns represent the attributes of the entity. For instance, a ‘list of employees of a company is an entity where employee name, ID, address, etc., are the attributes of the entity.  A relationship indicates how entities in a database are related to each other. Simply put, how a row in a table is related to row(s) of another table in a database. The relationship is made using the primary key and the foreign key primarily. There are three types of relationships in DBMS, as mentioned below: One-to-one relationship One-to-many relationship Many-to-many relationship."
"What is a Trigger, and mention its various types?","Triggers are nothing but they are special stored procedures. When there is an event in the SQL server, triggers will be fired automatically. There are three types of triggers – LOGON, DDL, and DML. LOGON triggers: They get fired when a user starts a Logon event. DDL triggers: They get fired when there is a DDL event. DML Triggers: They get fired when there is a modification in data due to DML."
"What is Schema in SQL, and mention its advantages?","The schema represents the logical structures of data. Using schemas, the database objects can be grouped logically in a database. Schema is useful for segregating database objects based on different applications, controlling access permissions, and managing a database's security aspects. Simply out, Schemas ensure database security and consistency. Advantages: Schemas can be easily transferred You can transfer database objects between schemas It protects database objects and achieves effective access control."
What are the types of UDFs?,There are three types of UDFs. They are defined as follows: User-defined scalar functions Table-valued functions System functions.
What is the difference between char and varchar data types?,"Char data type is a fixed-length data type in which the length of the character cannot be changed during execution. It supports storing normal and alphanumeric characters. On the other hand, varchar is the variable-length data type in which the length of the character can be changed during execution. That's why, it is known as a dynamic data type."
Mention the Aggregate Functions used in SQL?,The following aggregate functions are used in SQL. COUNT SUM AVG MAX MIN.
What are Case Manipulation Functions used in SQL?,The following are the case manipulation functions used in SQL. LOWER UPPER INITCAP.
What are Character Manipulation Functions used in SQL?,The following are the character manipulation functions used in SQL. CONCAT SUBSTR LENGTH INSTR LPAD RPAD TRIM REPLACE.
